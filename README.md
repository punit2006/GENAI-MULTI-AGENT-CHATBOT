# Groq AI Agent Chatbot (LangGraph + LangChain)

A conversational AI chatbot built using **Groq API**, **LangChain**, **LangGraph**, and **Gradio** — fully runnable in **Google Colab**.  
This project is based on the architecture of *AIwithHassan’s FastAPI LangGraph chatbot*, restructured into a notebook-friendly format that requires no backend deployment.

***

## 🚀 Features

- Powered by **Groq LPU** models (Llama 3.3 70B Versatile or Mixtral 8x7B)
- Built using **LangGraph** for agent orchestration  
- Integrated with **Tavily Search API** for real-time web queries  
- Includes a **custom math reasoning tool** via `LLMMathChain`  
- Interactive **Gradio** chat interface directly in Colab  
- Designed for **lightweight, reproducible, and zero-server setups**

***

## 🧠 Tech Stack

| Component | Description |
|------------|-------------|
| **Groq API** | Fast and low-latency inference engine for LLMs |
| **LangChain & LangGraph** | Orchestrates multi-step reasoning and agent workflows |
| **Gradio** | Web UI for live chatbot interaction |
| **Tavily API** | Enables contextual search capabilities |
| **Python 3.10+** | Recommended runtime environment |

***

## 📋 Prerequisites

Before running the notebook:

1. Create a **Groq Cloud** account at [https://console.groq.com](https://console.groq.com)  
   Generate an API key and copy it.
2. Create a **Tavily** account from [https://tavily.com](https://tavily.com)  
   Generate the Tavily API key.
3. Have access to **Google Colab** (recommended).

***

## ⚙️ Installation & Setup

Run the following inside a Google Colab cell:

```bash
!pip install langchain langgraph groq tavily-python gradio python-dotenv langchain-groq langchain-community
```

***

## 🧩 Code Example

```python
# ==================================
# Imports
# ==================================
import os
import gradio as gr
from langchain_groq import ChatGroq
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool

# ==================================
# Set API Keys
# ==================================
os.environ["GROQ_API_KEY"] = "PASTE_API_KEY_HERE"
os.environ["TAVILY_API_KEY"] = "PASTE_API_KEY_HERE"

# ==================================
# Initialize Groq LLM
# ==================================
llm = ChatGroq(
    temperature=0.6,
    model="llama-3.3-70b-versatile"  # Switchable to 'llama3-70b-8192'
)

# ==================================
# Define Tools
# ==================================
tavily_tool = TavilySearchResults(max_results=5)

@tool
def llm_math(query: str):
    """Solve math problems using the LLM."""
    from langchain.chains import LLMMathChain
    llm_math_chain = LLMMathChain.from_llm(llm)
    return llm_math_chain.run(query)

tools = [tavily_tool, llm_math]

# ==================================
# Initialize LangGraph Agent
# ==================================
agent = create_react_agent(llm, tools)

# ==================================
# Chat Function
# ==================================
def groq_chatbot(query):
    try:
        response = agent.invoke({"messages": [("user", query)]})
        return response['messages'][-1].content
    except Exception as e:
        return f"Error: {e}"

# ==================================
# Launch Gradio Interface
# ==================================
ui = gr.Interface(
    fn=groq_chatbot,
    inputs="text",
    outputs="text",
    title="Groq AI Agent Chatbot (LangGraph + LangChain)",
    description="Conversational AI agent powered by Groq, LangGraph, and Tavily."
)
ui.launch(debug=True)
```

***

## 🧪 Running on Google Colab

1. Open a new notebook in [Google Colab](https://colab.research.google.com/).  
2. Copy & paste the entire code block above into a single cell.  
3. Replace environment placeholders (`PASTE_API_KEY_HERE`) with your actual keys.  
4. Run the cell and click the link generated by **Gradio** to start chatting.

***

## 🧰 Customization

| Customization | Description |
|---------------|-------------|
| **Model** | Change `model="llama-3.3-70b-versatile"` to other Groq-supported models |
| **Temperature** | Adjust creativity (0.0–1.0) |
| **Max Tavily results** | Change `max_results=5` |
| **Add more tools** | Implement new LangChain tools for APIs, databases, etc. |

***

## 🧑‍💻 Example Queries

Try interacting with the chatbot using natural language:

- “Summarize the latest AI research trends.”  
- “Solve 245 ÷ 7 × (3 + 2).”  
- “Search what’s new with LangGraph 0.2 release.”

***

## 🛠️ Troubleshooting

| Issue | Solution |
|--------|-----------|
| `ModuleNotFoundError: langchain_groq` | Run `!pip install langchain-groq` again |
| `No content in response` | Check that your Groq API key is active and correct |
| `Tavily API errors` | Verify your Tavily API key and quota |

***

## 📄 License

This project is licensed under the **MIT License** — free for personal and commercial use.

***

## 🤝 Acknowledgments

- **AIwithHassan** for inspiring the original FastAPI LangGraph chatbot structure  
- **Groq** for high-performance inference through their API  
- **LangChain & LangGraph** developers for robust and modular frameworks  
- **Tavily API** for seamless real-time web search  
- **Gradio** for the effortless UI experience
